{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_nGQ9Ps842E"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Auto Train YOLOv8 Model with Autodistill\n",
        "\n",
        "Autodistill uses big, slower foundation models to train small, faster supervised models. Using `autodistill`, you can go from unlabeled images to inference on a custom model running at the edge with no human intervention in between.\n",
        "\n",
        "![Autodistill Steps](https://media.roboflow.com/open-source/autodistill/steps.jpg)\n",
        "\n",
        "As foundation models get better and better they will increasingly be able to augment or replace humans in the labeling process. We need tools for steering, utilizing, and comparing these models. Additionally, these foundation models are big, expensive, and often gated behind private APIs. For many production use-cases, we need models that can run cheaply and in realtime at the edge.\n",
        "\n",
        "![Autodistill Connections](https://media.roboflow.com/open-source/autodistill/connections.jpg)\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Before you start\n",
        "- Image dataset preperation\n",
        "- Autolabel dataset\n",
        "- Train target model\n",
        "- Evaluate target model\n",
        "- Run video inference\n",
        "- Upload dataset and model to Roboflow (comming soon)\n",
        "\n",
        "## üî• Let's begin!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anyZ7uFz_HNQ"
      },
      "source": [
        "## ‚ö° Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nFmBOnd8vFv",
        "outputId": "f62fdc30-f883-4afe-a058-3d45cb1767a2"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8ai--6D_7rG"
      },
      "source": [
        "## üß™ Install autodistill\n",
        "\n",
        "**NOTE:** Autodistill is an ecosystem for using big, slower foundation models to train small, faster supervised models. Each Base, as well as the Target model, has its own separate repository and pip package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byokHef__Jfd",
        "outputId": "f46f49af-20a9-43ad-c155-3a4f3f84b415"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "autodistill \\\n",
        "autodistill-grounded-sam \\\n",
        "autodistill-yolov8 \\\n",
        "roboflow \\\n",
        "supervision==0.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import autodistill\n",
        "\n",
        "    print(\"autodistill installed successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Failed to import autodistill.\")\n",
        "\n",
        "try:\n",
        "    import autodistill_grounded_sam\n",
        "\n",
        "    print(\"autodistill-grounded-sam installed successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Failed to import autodistill-grounded-sam.\")\n",
        "\n",
        "try:\n",
        "    import autodistill_yolov8\n",
        "\n",
        "    print(\"autodistill-yolov8 installed successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Failed to import autodistill-yolov8.\")\n",
        "\n",
        "try:\n",
        "    import roboflow\n",
        "\n",
        "    print(\"roboflow installed successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Failed to import roboflow.\")\n",
        "\n",
        "try:\n",
        "    import supervision\n",
        "\n",
        "    print(\"supervision installed successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Failed to import supervision.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI8XZLnxA0D6"
      },
      "source": [
        "**NOTE:** To make it easier for us to manage datasets, images and models we create a `HOME` constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udhh5dN_Ai54",
        "outputId": "b9fb472b-9ace-4a97-8685-0a1dfb798f3b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNBxRPlW-12t"
      },
      "source": [
        "## üñºÔ∏è Image dataset preperation\n",
        "\n",
        "**NOTE:** To use Autodistill all you need to have is a folder of images that you want to automatically annotate, and use for target model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7xzFhi6fwTd"
      },
      "source": [
        "Once you have downloaded your dataset, move all of the images from the `train` set in your downloaded dataset into the `images/` directory we created earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QvJmLyA5fwTd"
      },
      "outputs": [],
      "source": [
        "# %mv {HOME}/<dataset-name>/train/* {HOME}/images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhXomuTyfwTd"
      },
      "source": [
        "Now we are ready to start using Autodistill!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF16BQIWWwfb"
      },
      "source": [
        "### Download raw videos\n",
        "\n",
        "**NOTE:** In this tutorial, we will start with a directory containing video files and I will show you how to turn it into a ready-to-use collection of images. If you are working with your images, you can skip this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAI8n81PO08E"
      },
      "source": [
        "### Convert videos into images\n",
        "\n",
        "**NOTE:** Now, let's convert videos into images. By default, the code below saves every `10th` frame from each video. You can change this by manipulating the value of the `FRAME_STRIDE` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1_9OwJw0T1l2"
      },
      "outputs": [],
      "source": [
        "VIDEO_DIR_PATH = f\"{HOME}/videos\"\n",
        "IMAGE_DIR_PATH = f\"{HOME}/images\"\n",
        "FRAME_STRIDE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGrfasVvyomx"
      },
      "source": [
        "**NOTE:** Notice that we put two of our videos aside so that we can use them at the end of the notebook to evaluate our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt0DJTpvUIm5"
      },
      "source": [
        "### Display image sample\n",
        "\n",
        "**NOTE:** Before we start building a model with autodistill, let's make sure we have everything we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hKxoZuw2Mze",
        "outputId": "3b946c4a-a871-4c01-abdf-23769a468719"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "image_paths = sv.list_files_with_extensions(\n",
        "    directory=IMAGE_DIR_PATH,\n",
        "    extensions=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "print('image count:', len(image_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3mlA1Xw2ZdV"
      },
      "source": [
        "**NOTE:** We can also plot sample of our image dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eaBaWn-DUaec"
      },
      "outputs": [],
      "source": [
        "IMAGE_DIR_PATH = f\"{HOME}/images\"\n",
        "SAMPLE_SIZE = 6\n",
        "SAMPLE_GRID_SIZE = (3, 2)\n",
        "SAMPLE_PLOT_SIZE = (16, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QQEvtmvnN_ih",
        "outputId": "a4256fde-356a-48fe-cd42-d76171e0517a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import supervision as sv\n",
        "\n",
        "titles = [\n",
        "    image_path.stem\n",
        "    for image_path\n",
        "    in image_paths[:SAMPLE_SIZE]]\n",
        "images = [\n",
        "    cv2.imread(str(image_path))\n",
        "    for image_path\n",
        "    in image_paths[:SAMPLE_SIZE]]\n",
        "\n",
        "sv.plot_images_grid(images=images, titles=titles, grid_size=SAMPLE_GRID_SIZE, size=SAMPLE_PLOT_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCMpPz3wVb_M"
      },
      "source": [
        "## üè∑Ô∏è Autolabel dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIVuc89kVp2w"
      },
      "source": [
        "### Define ontology\n",
        "\n",
        "**Ontology** - an Ontology defines how your Base Model is prompted, what your Dataset will describe, and what your Target Model will predict. A simple Ontology is the CaptionOntology which prompts a Base Model with text captions and maps them to class names. Other Ontologies may, for instance, use a CLIP vector or example images instead of a text caption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "with open(\"data/Semantic Map Specification.txt\", \"r\") as file:\n",
        "    content = file.read()\n",
        "names = re.findall(r\"name=([^\\n]+)\", content)\n",
        "# lowercase names and remove _\n",
        "names = [name.lower().replace(\"_\", \" \") for name in names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24qFSVyhUV8C"
      },
      "outputs": [],
      "source": [
        "from autodistill.detection import CaptionOntology\n",
        "\n",
        "ont_list = {(f\"{name} wood defect\"): name for name in names}\n",
        "print(ont_list)\n",
        "ontology = CaptionOntology( ont_list )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXyoVtz_WGYq"
      },
      "source": [
        "### Initiate base model and autolabel\n",
        "\n",
        "**Base Model** - A Base Model is a large foundation model that knows a lot about a lot. Base models are often multimodal and can perform many tasks. They're large, slow, and expensive. Examples of Base Models are GroundedSAM and GPT-4's upcoming multimodal variant. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6ZGWXyYXWSSj"
      },
      "outputs": [],
      "source": [
        "DATASET_DIR_PATH = f\"{HOME}/dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY_jZbAG4ZMi"
      },
      "source": [
        "**NOTE:** Base Models are slow... Make yourself a coffee, autolabeing may take a while. ‚òï"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Set the TORCH_HOME environment variable to a directory on drive D\n",
        "os.environ[\"TORCH_HOME\"] = \"D:/torch_cache\"\n",
        "\n",
        "# Verify the setting\n",
        "print(f\"TORCH_HOME is set to: {os.environ['TORCH_HOME']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save copy of .bmp as .png\n",
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(IMAGE_DIR_PATH):\n",
        "    for file in files:\n",
        "        if file.endswith('.bmp'):\n",
        "            img = cv2.imread(os.path.join(root, file))\n",
        "            cv2.imwrite(os.path.join(root, file.replace('.bmp', '.png')), img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jmJS9aJV5VW"
      },
      "outputs": [],
      "source": [
        "from autodistill_grounded_sam import GroundedSAM\n",
        "\n",
        "base_model = GroundedSAM(ontology=ontology)\n",
        "dataset = base_model.label(\n",
        "    input_folder=IMAGE_DIR_PATH,\n",
        "    extension=\".bmp\",\n",
        "    output_folder=DATASET_DIR_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM9zH9BGZGp0"
      },
      "source": [
        "### Display dataset sample\n",
        "\n",
        "**Dataset** - a Dataset is a set of auto-labeled data that can be used to train a Target Model. It is the output generated by a Base Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XsTHheE-bMbY"
      },
      "outputs": [],
      "source": [
        "ANNOTATIONS_DIRECTORY_PATH = f\"{HOME}\\dataset\\labels\"\n",
        "IMAGES_DIRECTORY_PATH = f\"{HOME}\\dataset\\images\"\n",
        "DATA_YAML_PATH = f\"{HOME}\\dataset\\data.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print paths\n",
        "print(ANNOTATIONS_DIRECTORY_PATH)\n",
        "print(IMAGES_DIRECTORY_PATH)\n",
        "print(DATA_YAML_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhgy9rbdYSCZ",
        "outputId": "afc61564-c368-4532-c610-2c3bec1abc72"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "dataset = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=IMAGES_DIRECTORY_PATH,\n",
        "    annotations_directory_path=ANNOTATIONS_DIRECTORY_PATH,\n",
        "    data_yaml_path=DATA_YAML_PATH)\n",
        "\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "krNCDdDZcm7H",
        "outputId": "51b26cc1-7884-4bd3-e118-d69106cab0a1"
      },
      "outputs": [],
      "source": [
        "import supervision as sv\n",
        "\n",
        "image_names = list(dataset.images.keys())[:SAMPLE_SIZE]\n",
        "\n",
        "mask_annotator = sv.MaskAnnotator()\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "\n",
        "images = []\n",
        "for image_name in image_names:\n",
        "    image = dataset.images[image_name]\n",
        "    annotations = dataset.annotations[image_name]\n",
        "    labels = [\n",
        "        dataset.classes[class_id]\n",
        "        for class_id\n",
        "        in annotations.class_id]\n",
        "    annotates_image = mask_annotator.annotate(\n",
        "        scene=image.copy(),\n",
        "        detections=annotations)\n",
        "    annotates_image = box_annotator.annotate(\n",
        "        scene=annotates_image,\n",
        "        detections=annotations,\n",
        "        labels=labels)\n",
        "    images.append(annotates_image)\n",
        "\n",
        "sv.plot_images_grid(\n",
        "    images=images,\n",
        "    titles=image_names,\n",
        "    grid_size=SAMPLE_GRID_SIZE,\n",
        "    size=SAMPLE_PLOT_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ9ESdTCagkm"
      },
      "source": [
        "## üî• Train target model - YOLOv8\n",
        "\n",
        "**Target Model** - a Target Model is a supervised model that consumes a Dataset and outputs a distilled model that is ready for deployment. Target Models are usually small, fast, and fine-tuned to perform a specific task very well (but they don't generalize well beyond the information described in their Dataset). Examples of Target Models are YOLOv8 and DETR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sLzt1MdalB9"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "from autodistill_yolov8 import YOLOv8\n",
        "\n",
        "target_model = YOLOv8(\"yolov8n.pt\")\n",
        "target_model.train(DATA_YAML_PATH, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlv5QA3Vg06Z",
        "outputId": "1767c48d-64b8-4da3-e1f4-ef1a7d68bf8f"
      },
      "outputs": [],
      "source": [
        "!ls {HOME}/runs/detect/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Tn0fgNgoQl"
      },
      "source": [
        "## ‚öñÔ∏è Evaluate target model\n",
        "\n",
        "**NOTE:** As with the regular YOLOv8 training, we can now take a look at artifacts stored in `runs` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "z8EW50NAgSdU",
        "outputId": "336bf84d-1e38-49de-d001-3de817897078"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "jwCnrPUYhIrE",
        "outputId": "771018e2-efbd-4b01-e605-931b5702e0bd"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f'{HOME}/runs/detect/train/results.png', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "Xy3lqKL7hRTD",
        "outputId": "eeb97b37-50f7-4d5d-9751-c6ffce121570"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltEgqJsnjPA6"
      },
      "source": [
        "## üé¨ Run Inference on a video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCV12io6jZZh"
      },
      "outputs": [],
      "source": [
        "INPUT_VIDEO_PATH = TEST_VIDEO_PATHS[0]\n",
        "OUTPUT_VIDEO_PATH = f\"{HOME}/output.mp4\"\n",
        "TRAINED_MODEL_PATH = f\"{HOME}/runs/detect/train/weights/best.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCDEjx4wnHJy"
      },
      "outputs": [],
      "source": [
        "!yolo predict model={TRAINED_MODEL_PATH} source={INPUT_VIDEO_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m243-_GV6dE8"
      },
      "source": [
        "## Upload dataset and model to Roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7ZJrRkw6jqE"
      },
      "source": [
        "comming soon..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWpUXXkyhaq3"
      },
      "source": [
        "  # üèÜ Congratulations\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000ec4a942dc4fceac816501fe6436a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "177f20cb500d4dc7ad338e844897c735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43df62c36cec4dfaba5203164756f692",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f19fe218fc5940b490dbd9d4dfae89f9",
            "value": " 6/6 [01:25&lt;00:00, 12.37s/it]"
          }
        },
        "23636106a532455c8146205e4f7be2fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43df62c36cec4dfaba5203164756f692": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58be2b02981b4aedad5294cecbccc3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6250697a80024656af350a4c05d456bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90924baeb3644d45bbfdfb9f7b0520dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1f33b8580474febb5ffe5be8442e090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec38cea329c64274a2747e7387b8ae3c",
              "IPY_MODEL_c3cff42073644b91863056fe6a2d8f34",
              "IPY_MODEL_177f20cb500d4dc7ad338e844897c735"
            ],
            "layout": "IPY_MODEL_23636106a532455c8146205e4f7be2fe"
          }
        },
        "c3cff42073644b91863056fe6a2d8f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000ec4a942dc4fceac816501fe6436a7",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90924baeb3644d45bbfdfb9f7b0520dc",
            "value": 6
          }
        },
        "ec38cea329c64274a2747e7387b8ae3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6250697a80024656af350a4c05d456bc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_58be2b02981b4aedad5294cecbccc3be",
            "value": "100%"
          }
        },
        "f19fe218fc5940b490dbd9d4dfae89f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
